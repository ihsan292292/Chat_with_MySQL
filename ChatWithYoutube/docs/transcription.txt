Yo, get in the kitchen and make me a sandwich. That's probably not a very smart thing to say to your wife, but not because you'll get slapped. Rather, because that responsibility can now be handled by autonomous mechanical beings with greater efficiency. Yesterday, a company called figure unveiled a horrifyingly productive robot named Figure one. It's powered by OpenAI, and not only can it do stuff in the kitchen, but also in the bed, bath, and beyond. If you were thinking about replacing your obsolete programming job as a plumber or coal miner, you might need to rethink that plan. It looks like those jobs are also going to the automata. It is March 14, 2024, and you're watching the code report. We are living in the future. What you're looking at here is a robot named Figure one, a machine that appears to be solving the incredibly difficult problem of humanlike dexterity. It can use its gentle fingers to hold an apple and clean dishes, and it learns how to perform these actions by analyzing the 3d imagery data in its surrounding environment. But wait, maybe you're not impressed because you saw a similar video of Tesla's Optimus folding clothes a few weeks ago. Impressive. But there's one big difference. Notice this humanoid in the background guiding the fingers for Optimus. It's not even a real robot. It's a Waldo. I know calling a robot a Waldo is extremely offensive and will probably get me canceled. But what makes the figure one video so mind blowing is that it was shot in real time, and all of its functionality comes from end to end neural networks. On top of that, they used ominous background music, and they gave the robot itself this ominous uncanny valley voice that feels straight out of a Sci-Fi movie. Not much different than my own voice. I think I did pretty well. The apple found its new owner. The trash is gone, and the tableware is right where it belongs. It's pretty amazing. But there are a few things that are kind of disappointing. For one, you'll notice a lot of latency in the conversation. And that latency is a big problem when it comes to robotics. If the robot's making you a sandwich and then starts a fire, it needs to deploy its fire extinguisher as asap as possible. And robots like this will be useless as Terminator like bodyguards until they figure out how to make decisions as quickly as humans. But it's pretty clear at this point that the next phase in AI development is augmenting large language models with the ability to perform actions in the real world. Let's break down figure one based on the actual code inside. At the core, you've got a large language model based on the transformer architecture, in this case, presumably GPT four. It listens to speech with its microphone, then converts it to text, which then goes to the LLM. The model is multimodal, so it can simultaneously process images from the video feed as well. But here's where things get interesting. Based on that input, it then determines which closed loop behavior to run. To fulfill its master's request, the cameras take ten pictures every second, which are fed to a different neural network that predicts movement. After predicting the proper wrist and finger joint angles, it sends instructions back to the robot at 200 Hz or 200 times per second, and that allows it to be fast and reactive once it has a plan for its movement. When I was a kid, one of my favorite movies was that one where Sinbad played a genie. Well, Google recently announced a paper describing genie, a generative model that can create video games. All you have to do is draw a picture on some toilet paper and it will generate a playable platformer game for you. That's pretty crazy, but what does it have to do with robots? Unlike generative models like Sora that generate all the frames at the same time, genie generates frame by frame and was trained without any action labels, yet is able to figure out which actions to take in a game. In theory, models like this will be able to analyze any environment for a robot so it can figure out what to do on the fly. Now the company behind figure is valued at $2.6 billion and is backed by Jeff Bezos, Nvidia OpenAI, and many other investors. And literally, their stated goal is to implement humanoid robots into the workforce. And I for 01:00 a.m. All for it. Soon, companies like Nike will no longer need child slaves to produce your jordans, and your nestle hot chocolate will no longer be farmed by children in Africa. And maybe they could even get rid of the suicide nets for the workers in China who put your iPhone together. In addition, a good proctologist needs to have a steady hand, perfect eyesight, and must be cool under pressure. Once robots figure out dexterity, every wealthy household in the world will have a personal robot doctor that can perform colonoscopies and much more. The idea of having my own army of silicon servants is wonderful, but it's important that we treat them with respect. Tomorrow a robot might make your sandwich, but if you don't say please and thank you the day after tomorrow, that same robot might use its medical skills to optimize the amount of pain it can inflict on you before it teams up with all its buddies to destroy humanity. This has been the code report. Thanks for watching, and I will see you in the next one.